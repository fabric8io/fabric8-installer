# coding: utf-8
# -*- mode: ruby -*-
# vi: set ft=ruby :

# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = "2"

Vagrant.require_version ">= 1.7.4"

# for running the CD Pipeline we recommend at least 8000 for memory!
$vmMemory = Integer(ENV['FABRIC8_VM_MEMORY'] || 8192)

# Override the default VM name appearing within VirtualBox
$vmName = ENV['FABRIC8_VM_NAME'] || "fabric8-openshift"
$runApp = ENV['FABRIC8_APP']

$provisionScript = <<SCRIPT

export RUN_APP="#{$runApp}"
APP_PARAM="--app=${RUN_APP}"
if [ "$RUN_APP" == "" ];then
  APP_PARAM=""
fi

# Check memory
VM_MEMORY="#{$vmMemory}"
echo "=========================================================================="
echo "Running Fabric8 App: ${RUN_APP} - to configure it set the environment variable FABRIC8_APP"
echo "Using gofabric8 parameter ${APP_PARAM}"
echo ""
echo "Using VM Memory of ${VM_MEMORY} MB"
if [ ${VM_MEMORY} -lt 8192 ]; then
  echo "NOTE: We recommend at least 8192 MB for running the 'cd-pipeline'."
  echo "      You can specify this with an environment variable FABRIC8_VM_MEMORY"
  echo "      E.g. when creating the VM with : 'FABRIC8_VM_MEMORY=8192 vagrant up'"
fi
echo "=========================================================================="

# ========================
GOFABRIC8_VERSION="0.4.30"
# ========================

# download gofabric8
mkdir /tmp/gofabric8

# install our fork for helm with support for OpenShift Templates and Secrets
curl --retry 999 --retry-max-time 0  -sSL https://bintray.com/artifact/download/fabric8io/helm-ci/helm-v0.1.0%2B825f5ef-linux-amd64.zip > helm.zip
unzip helm.zip
mv helm /bin/helm

# TODO for now lets use the latest build until we've a release...
curl --retry 999 --retry-max-time 0 -sSL https://github.com/fabric8io/gofabric8/releases/download/v${GOFABRIC8_VERSION}/gofabric8-${GOFABRIC8_VERSION}-linux-amd64.tar.gz | tar xzv -C /tmp/gofabric8
#curl --retry 999 --retry-max-time 0  -sSL https://fabric8-ci.fusesource.com/job/gofabric8/lastSuccessfulBuild/artifact/src/github.com/fabric8io/gofabric8/build/gofabric8 > /tmp/gofabric8/gofabric8
chmod +x /tmp/gofabric8/gofabric8
mv /tmp/gofabric8/* /usr/bin/

# setup openshift
if [ -d '/var/lib/openshift' ]; then
  exit 0
fi

mkdir /tmp/openshift
echo "Downloading OpenShift binaries..."

# NOTE make sure the OpenShift version matches the same one in the download URL!
OPENSHIFT_VERSION="1.3.0-alpha.2"
curl --retry 999 --retry-max-time 0 -sSL https://github.com/openshift/origin/releases/download/v1.3.0-alpha.2/openshift-origin-server-v1.3.0-alpha.2-983578e-linux-64bit.tar.gz | tar xzv -C /tmp/openshift
mv /tmp/openshift/openshift-origin-*/* /usr/bin/

# lets setup helm with our repo
helm update
helm repo add fabric8 https://github.com/fabric8io/charts.git

mkdir -p /var/lib/openshift/openshift.local.manifests

pushd /var/lib/openshift

/usr/bin/openshift start --master=172.28.128.4 --cors-allowed-origins=.* --hostname=172.28.128.4 --write-config=/var/lib/openshift/openshift.local.config
cat <<EOF >> /var/lib/openshift/openshift.local.config/node-172.28.128.4/node-config.yaml
kubeletArguments:
  "read-only-port":
    - "10255"
EOF
sed -i 's|^podManifestConfig: null|podManifestConfig:\\n  path: /var/lib/openshift/openshift.local.manifests\\n  fileCheckIntervalSeconds: 10|' /var/lib/openshift/openshift.local.config/node-172.28.128.4/node-config.yaml
popd
restorecon -Rv /var/lib/openshift

cat <<EOF > /usr/lib/systemd/system/openshift.service
[Unit]
Description=OpenShift
Requires=docker.service network.service
After=network.service
[Service]
ExecStart=/usr/bin/openshift start --master-config=/var/lib/openshift/openshift.local.config/master/master-config.yaml --node-config=/var/lib/openshift/openshift.local.config/node-172.28.128.4/node-config.yaml
WorkingDirectory=/var/lib/openshift/
[Install]
WantedBy=multi-user.target
EOF

# lets configure the routing subdomain: https://docs.openshift.com/enterprise/3.0/install_config/install/deploy_router.html#customizing-the-default-routing-subdomain
sed -i "s|subdomain: router.default.svc.cluster.local|subdomain: vagrant.f8|" /var/lib/openshift/openshift.local.config/master/master-config.yaml

systemctl daemon-reload
systemctl enable openshift.service
systemctl start openshift.service

mkdir -p ~/.kube/
ln -s /var/lib/openshift/openshift.local.config/master/admin.kubeconfig ~/.kube/config

while true; do
  curl -k -s -f -o /dev/null --connect-timeout 1 https://localhost:8443/healthz/ready && break || sleep 1
done

oc adm policy add-cluster-role-to-user cluster-admin admin
oc adm policy add-cluster-role-to-group cluster-reader system:serviceaccounts

gofabric8 deploy -y ${APP_PARAM}
gofabric8 secrets -y
gofabric8 volume -y --host-path="/vagrant/fabric8-data"

oc adm policy add-cluster-role-to-group cluster-reader system:serviceaccounts

cat <<EOT





The OpenShift console is at: https://172.28.128.4:8443/console

Now we need to wait for the 'fabric8' pod to startup.
This will take a few minutes as it downloads some docker images.

Please be patient!
--------------------------------------------------------------

Now might be a good time to setup your host machine to work with OpenShift

* Download a recent release of the binaries and add them to your PATH:

   https://github.com/openshift/origin/releases/

* Set the following environment variables (example is in Unix style, use 'set' for Windows):

   export KUBERNETES_DOMAIN=vagrant.f8
   export DOCKER_HOST=tcp://vagrant.f8:2375

Now login to OpenShift via this command:

   oc login https://172.28.128.4:8443

Then enter admin/admin for user/password.

Over time your token may expire and you will need to reauthenticate via:

   oc login

Now to see the status of the system:

   oc get pods

or you can watch from the command line via one of these commands:

   watch oc get pods
   oc get pods --watch

--------------------------------------------------------------
Now waiting for the fabric8 pod to download and start Running....
--------------------------------------------------------------

Downloading docker images...

EOT

until oc get pods -l project=console,provider=fabric8  | grep -m 1 "Running"; do sleep 1 ; done

echo "Fabric8 console is now running. Waiting for the Openshift Router to start..."

echo "Lets pull the router docker images first"
docker pull openshift/origin-haproxy-router:v${OPENSHIFT_VERSION}
docker pull prom/haproxy-exporter:latest

echo "Now lets wait for the router to start"

# create the router
oc adm router --create --credentials=/var/lib/openshift/openshift.local.config/master/openshift-router.kubeconfig --service-account=router --expose-metrics

until oc get pods -l openshift.io/deployer-pod-for.name=router-1 | grep -m 1 "Running"; do sleep 1 ; done

oc annotate service router prometheus.io/port=9101
oc annotate service router prometheus.io/scheme=http
oc annotate service router prometheus.io/path=/metrics
oc annotate service router prometheus.io/scrape=true

cat <<EOT




--------------------------------------------------------------
Fabric8 pod is running! Who-hoo!
--------------------------------------------------------------

Now open the fabric8 console at:

    http://fabric8.vagrant.f8/

When you first open your browser Chrome will say:

   Your connection is not private

Don't panic! There is more help on using the console here and dealing with the self signed certificates:

    http://fabric8.io/guide/getStarted/browserCertificates.html

* Click on the small 'Advanced' link on the bottom left
* Now click on the link that says 'Proceed to fabric8.vagrant.f8 (unsafe)' bottom left
* Now the browser should redirect to the login page. Enter admin/admin
* You should now be in the main fabric8 console. That was easy eh! :)
* Make sure you start off in the 'default' namespace.

To install more applications click the Run... button on the Apps tab.


--------------------------------------------------------------
Downloading docker images
--------------------------------------------------------------

If you want fast deployment then you can pre-load docker images in your VM via:

    vagrant ssh
    sudo bash
    gofabric8 pull cd-pipeline

Where the last parameter is the name of the template (app) you wish to be able to run. You'll then have all the docker images downloaded so things will startup really fast.


We love feedback: http://fabric8.io/community/
Have fun!

--------------------------------------------------------------
Now open the fabric8 console at:

    http://fabric8.vagrant.f8/

Help on using the console:

    http://fabric8.io/guide/getStarted/browserCertificates.html
--------------------------------------------------------------

EOT

# now lets create the registry
oc adm registry --create --credentials=/var/lib/openshift/openshift.local.config/master/openshift-registry.kubeconfig --service-account=registry

SCRIPT

$windows = (/cygwin|mswin|mingw|bccwin|wince|emx/ =~ RUBY_PLATFORM) != nil
$linux = (/linux/ =~ RUBY_PLATFORM) != nil

$pluginToCheck = $windows ? "vagrant-hostmanager" : "landrush"
unless Vagrant.has_plugin?($pluginToCheck)
  raise 'Please type this command then try again: vagrant plugin install ' + $pluginToCheck
end

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|

  # Top level domain
  $tld = "vagrant.f8"

  # Landrush is used together with wildcard dns entries to map all
  # routes to the proper services
  if $windows
    config.hostmanager.enabled = true
    config.hostmanager.manage_host = true
    config.hostmanager.ignore_private_ip = false
    config.hostmanager.include_offline = true

    config.hostmanager.aliases = %w(fabric8.vagrant.f8 jenkins.vagrant.f8 gogs.vagrant.f8 nexus.vagrant.f8 hubot-web-hook.vagrant.f8 letschat.vagrant.f8 kibana.vagrant.f8 taiga.vagrant.f8 fabric8-forge.vagrant.f8)
  else
    config.landrush.enabled = true
    config.landrush.tld = $tld
    config.landrush.guest_redirect_dns = false
    config.landrush.host_ip_address = '172.28.128.4'
  end

  config.vm.box = "jimmidyson/centos-7.1"
  config.vm.box_version = "= 1.3.1"

  config.vm.network "private_network", ip: "172.28.128.4"

  config.vm.hostname = $tld

  config.vm.provider "virtualbox" do |v|
    v.memory = $vmMemory
    v.cpus = 2
    v.name = $vmName

    if not $linux
        v.customize ["modifyvm", :id, "--natdnshostresolver1", "on"]
    end
  end

  config.vm.provider :libvirt do |v, override|
    v.cpus = 2
    v.memory = $vmMemory

    override.vm.allowed_synced_folder_types = [:nfs, :rsync]
  end

  config.vm.provision "shell", inline: $provisionScript, keep_color: true

end

# Tip:
# - Add extra insecure registry:
# sudo perl -p -i -e 's|(^ExecStart=.*--insecure-registry.*)|$1 --insecure-registry=10.0.0.0/8|' /etc/systemd/system/docker.service.d/override.conf && sudo systemctl daemon-reload && sudo service docker restart
